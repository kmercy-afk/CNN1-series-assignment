{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, shape):\n",
    "        # Xavier initialization: std = sqrt(1 / n), where n is the number of input units\n",
    "        n_in = shape[0]\n",
    "        stddev = np.sqrt(1. / n_in)\n",
    "        return np.random.normal(0, stddev, size=shape)\n",
    "\n",
    "class SimpleConv1d:\n",
    "    def __init__(self, input_size, filter_size, initializer=XavierInitializer()):\n",
    "        self.input_size = input_size  # Length of the input sequence\n",
    "        self.filter_size = filter_size  # Size of the filter\n",
    "        self.initializer = initializer\n",
    "        self.weights = self.initializer.initialize((filter_size, 1))  # Filter weights (1 channel)\n",
    "        self.bias = np.zeros(1)  # Bias (scalar)\n",
    "        self.output_size = input_size - filter_size + 1  # Output size, assuming no padding and stride=1\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = np.zeros(self.output_size)\n",
    "\n",
    "        # Perform convolution\n",
    "        for i in range(self.output_size):\n",
    "            self.output[i] = np.sum(self.X[i:i + self.filter_size] * self.weights.flatten()) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output, learning_rate):\n",
    "        # Initialize gradients\n",
    "        d_weights = np.zeros_like(self.weights)\n",
    "        d_bias = 0\n",
    "        d_input = np.zeros_like(self.X)\n",
    "\n",
    "        # Compute gradients for weights, bias, and input\n",
    "        for i in range(self.output_size):\n",
    "            # Reshape input slice to match the shape of weights (filter_size, 1)\n",
    "            input_slice = self.X[i:i + self.filter_size].reshape(-1, 1)\n",
    "            d_weights += d_output[i] * input_slice\n",
    "            d_bias += d_output[i]\n",
    "            # Compute gradient for input (error propagation)\n",
    "            for s in range(self.filter_size):\n",
    "                if 0 <= i - s < len(d_input):\n",
    "                    d_input[i - s] += d_output[i] * self.weights[s]\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "\n",
    "        return d_input  # Return the error to propagate backwards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [ -2.74465739  -4.16837041  -5.59208343  -7.01579645  -8.43950947\n",
      "  -9.86322249 -11.28693551 -12.71064853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_14168\\3194059677.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.output[i] = np.sum(self.X[i:i + self.filter_size] * self.weights.flatten()) + self.bias\n",
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_14168\\3194059677.py:46: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  d_input[i - s] += d_output[i] * self.weights[s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input (1D array)\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Create a SimpleConv1d layer with input size 10, filter size 3\n",
    "conv_layer = SimpleConv1d(input_size=10, filter_size=3)\n",
    "\n",
    "# Forward pass\n",
    "output = conv_layer.forward(X)\n",
    "print(\"Output:\", output)\n",
    "\n",
    "# Backward pass (dummy gradient)\n",
    "d_output = np.random.randn(len(output))  # Random gradient for demonstration\n",
    "conv_layer.backward(d_output, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: 10\n"
     ]
    }
   ],
   "source": [
    "def calculate_output_size(N_in, P, F, S):\n",
    "    \"\"\"\n",
    "    Calculate the output size of a 1D convolution layer.\n",
    "\n",
    "    Parameters:\n",
    "    - N_in: Input size (number of features)\n",
    "    - P: Padding size\n",
    "    - F: Filter size\n",
    "    - S: Stride size\n",
    "\n",
    "    Returns:\n",
    "    - N_out: Output size (number of features)\n",
    "    \"\"\"\n",
    "    N_out = (N_in + 2 * P - F) // S + 1\n",
    "    return N_out\n",
    "# Example parameters\n",
    "N_in = 10  # Input size\n",
    "P = 1      # Padding size\n",
    "F = 3      # Filter size\n",
    "S = 1      # Stride size\n",
    "\n",
    "# Calculate the output size\n",
    "N_out = calculate_output_size(N_in, P, F, S)\n",
    "print(\"Output size:\", N_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output: [35. 50.]\n",
      "Backward pass results:\n",
      "delta_b: 30\n",
      "delta_w: [ 50  80 110]\n",
      "delta_x: [ 30 110 170 140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_14168\\2896273398.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  a[0] = np.sum(x[indexes0] * w) + b\n",
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_14168\\2896273398.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  a[1] = np.sum(x[indexes1] * w) + b\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input, weights, and bias\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "\n",
    "# Forward pass\n",
    "a = np.empty(2)\n",
    "indexes0 = np.array([0, 1, 2]).astype(int)  # Use 'int' instead of 'np.int'\n",
    "indexes1 = np.array([1, 2, 3]).astype(int)  # Use 'int' instead of 'np.int'\n",
    "\n",
    "a[0] = np.sum(x[indexes0] * w) + b\n",
    "a[1] = np.sum(x[indexes1] * w) + b\n",
    "\n",
    "print(\"Forward pass output:\", a)\n",
    "\n",
    "# Backpropagation error (delta)\n",
    "delta_a = np.array([10, 20])\n",
    "\n",
    "# Calculate gradients\n",
    "delta_b = np.sum(delta_a)\n",
    "delta_w = np.zeros_like(w)\n",
    "delta_x = np.zeros_like(x)\n",
    "\n",
    "# Gradients for weights and input\n",
    "for i in range(len(delta_a)):\n",
    "    delta_w += delta_a[i] * x[indexes0[i]:indexes0[i] + len(w)]  # Accumulate gradients for weights\n",
    "    delta_x[indexes0[i]:indexes0[i] + len(w)] += delta_a[i] * w  # Error gradient w.r.t. input\n",
    "\n",
    "print(\"Backward pass results:\")\n",
    "print(\"delta_b:\", delta_b)\n",
    "print(\"delta_w:\", delta_w)\n",
    "print(\"delta_x:\", delta_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output:\n",
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "Backward pass:\n",
      "Gradient w.r.t input:\n",
      "[[ 55. 135. 135.  80.]\n",
      " [ 55. 135. 135.  80.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, input_channels, output_channels, filter_size):\n",
    "        # Initialize filters and bias\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.filter_size = filter_size\n",
    "        \n",
    "        # Filters: output_channels x input_channels x filter_size\n",
    "        self.w = np.ones((output_channels, input_channels, filter_size))  # Example: all ones for simplicity\n",
    "        \n",
    "        # Bias: output_channels (initialized as float64)\n",
    "        self.b = np.array([1, 2, 3], dtype=np.float64)  # Ensure bias is float64\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Conv1d layer\n",
    "        x: input data with shape (input_channels, feature_length)\n",
    "        Returns: output data with shape (output_channels, output_length)\n",
    "        \"\"\"\n",
    "        output_length = x.shape[1] - self.filter_size + 1\n",
    "        output = np.zeros((self.output_channels, output_length), dtype=np.float64)\n",
    "        \n",
    "        # Perform convolution for each output channel\n",
    "        for i in range(self.output_channels):\n",
    "            for j in range(output_length):\n",
    "                # Convolve the filter with the input x\n",
    "                for c in range(self.input_channels):\n",
    "                    output[i, j] += np.sum(x[c, j:j + self.filter_size] * self.w[i, c])\n",
    "                # Add bias for the output channel\n",
    "                output[i, j] += self.b[i]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, delta_a, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward pass through the Conv1d layer\n",
    "        x: input data with shape (input_channels, feature_length)\n",
    "        delta_a: gradient of the loss with respect to the output, shape (output_channels, output_length)\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        delta_w = np.zeros_like(self.w, dtype=np.float64)\n",
    "        delta_b = np.zeros_like(self.b, dtype=np.float64)\n",
    "        \n",
    "        # Ensure delta_x is initialized with the correct data type\n",
    "        delta_x = np.zeros_like(x, dtype=np.float64)\n",
    "        \n",
    "        # Gradients for weights, biases, and input\n",
    "        for i in range(self.output_channels):\n",
    "            for j in range(delta_a.shape[1]):\n",
    "                delta_b[i] += delta_a[i, j]\n",
    "                for c in range(self.input_channels):\n",
    "                    delta_w[i, c] += delta_a[i, j] * x[c, j:j + self.filter_size].sum()\n",
    "                    delta_x[c, j:j + self.filter_size] += delta_a[i, j] * self.w[i, c]\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.w -= learning_rate * delta_w\n",
    "        self.b -= learning_rate * delta_b\n",
    "        \n",
    "        return delta_x\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])  # Input: 2 channels, 4 features\n",
    "w = np.ones((3, 2, 3))  # Filters: 3 output channels, 2 input channels, filter size 3\n",
    "b = np.array([1, 2, 3])  # Biases: 3 output channels\n",
    "\n",
    "# Create Conv1d layer\n",
    "conv_layer = Conv1d(input_channels=2, output_channels=3, filter_size=3)\n",
    "\n",
    "# Forward pass\n",
    "output = conv_layer.forward(x)\n",
    "print(\"Forward pass output:\")\n",
    "print(output)\n",
    "\n",
    "# Backpropagation (dummy delta_a for demonstration)\n",
    "delta_a = np.array([[10, 20], [15, 25], [30, 35]])  # Dummy gradient of loss with respect to output\n",
    "delta_x = conv_layer.backward(x, delta_a, learning_rate=0.01)\n",
    "\n",
    "print(\"Backward pass:\")\n",
    "print(\"Gradient w.r.t input:\")\n",
    "print(delta_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output:\n",
      "[[ 9. 16. 22. 17.]\n",
      " [10. 17. 23. 18.]\n",
      " [11. 18. 24. 19.]]\n",
      "Backward pass:\n",
      "Gradient w.r.t input:\n",
      "[[135. 135.  80.   0.]\n",
      " [135. 135.  80.   0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, input_channels, output_channels, filter_size, padding=0):\n",
    "        # Initialize filters and bias\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Filters: output_channels x input_channels x filter_size\n",
    "        self.w = np.ones((output_channels, input_channels, filter_size))  # Example: all ones for simplicity\n",
    "        \n",
    "        # Bias: output_channels (initialized as float64)\n",
    "        self.b = np.array([1, 2, 3], dtype=np.float64)  # Ensure bias is float64\n",
    "        \n",
    "    def pad_input(self, x):\n",
    "        \"\"\"\n",
    "        Apply padding to the input array using np.pad().\n",
    "        Default padding is zero-padding.\n",
    "        \"\"\"\n",
    "        return np.pad(x, ((0, 0), (self.padding, self.padding)), mode='constant', constant_values=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Conv1d layer with padding\n",
    "        x: input data with shape (input_channels, feature_length)\n",
    "        Returns: output data with shape (output_channels, output_length)\n",
    "        \"\"\"\n",
    "        # Apply padding to the input\n",
    "        x_padded = self.pad_input(x)\n",
    "        output_length = x_padded.shape[1] - self.filter_size + 1\n",
    "        output = np.zeros((self.output_channels, output_length), dtype=np.float64)\n",
    "        \n",
    "        # Perform convolution for each output channel\n",
    "        for i in range(self.output_channels):\n",
    "            for j in range(output_length):\n",
    "                # Convolve the filter with the padded input x\n",
    "                for c in range(self.input_channels):\n",
    "                    output[i, j] += np.sum(x_padded[c, j:j + self.filter_size] * self.w[i, c])\n",
    "                # Add bias for the output channel\n",
    "                output[i, j] += self.b[i]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, delta_a, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward pass through the Conv1d layer with padding\n",
    "        x: input data with shape (input_channels, feature_length)\n",
    "        delta_a: gradient of the loss with respect to the output, shape (output_channels, output_length)\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        # Apply padding to the input for backpropagation\n",
    "        x_padded = self.pad_input(x)\n",
    "        \n",
    "        delta_w = np.zeros_like(self.w, dtype=np.float64)\n",
    "        delta_b = np.zeros_like(self.b, dtype=np.float64)\n",
    "        \n",
    "        # Ensure delta_x is initialized with the correct data type\n",
    "        delta_x = np.zeros_like(x_padded, dtype=np.float64)\n",
    "        \n",
    "        # Gradients for weights, biases, and input\n",
    "        for i in range(self.output_channels):\n",
    "            for j in range(delta_a.shape[1]):\n",
    "                delta_b[i] += delta_a[i, j]\n",
    "                for c in range(self.input_channels):\n",
    "                    delta_w[i, c] += delta_a[i, j] * x_padded[c, j:j + self.filter_size].sum()\n",
    "                    delta_x[c, j:j + self.filter_size] += delta_a[i, j] * self.w[i, c]\n",
    "        \n",
    "        # Remove padding from the gradient w.r.t input (delta_x)\n",
    "        delta_x = delta_x[:, self.padding:-self.padding] if self.padding > 0 else delta_x\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.w -= learning_rate * delta_w\n",
    "        self.b -= learning_rate * delta_b\n",
    "        \n",
    "        return delta_x\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])  # Input: 2 channels, 4 features\n",
    "w = np.ones((3, 2, 3))  # Filters: 3 output channels, 2 input channels, filter size 3\n",
    "b = np.array([1, 2, 3])  # Biases: 3 output channels\n",
    "\n",
    "# Create Conv1d layer with padding=1 (zero padding)\n",
    "conv_layer = Conv1d(input_channels=2, output_channels=3, filter_size=3, padding=1)\n",
    "\n",
    "# Forward pass\n",
    "output = conv_layer.forward(x)\n",
    "print(\"Forward pass output:\")\n",
    "print(output)\n",
    "\n",
    "# Backpropagation (dummy delta_a for demonstration)\n",
    "delta_a = np.array([[10, 20], [15, 25], [30, 35]])  # Dummy gradient of loss with respect to output\n",
    "delta_x = conv_layer.backward(x, delta_a, learning_rate=0.01)\n",
    "\n",
    "print(\"Backward pass:\")\n",
    "print(\"Gradient w.r.t input:\")\n",
    "print(delta_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, input_channels, output_channels, filter_size, padding=0):\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize filters and biases\n",
    "        self.w = np.ones((output_channels, input_channels, filter_size))  # All filters set to 1 for simplicity\n",
    "        self.b = np.array([1, 2, 3], dtype=np.float64)  # Biases initialized as floats\n",
    "        \n",
    "    def pad_input(self, x):\n",
    "        \"\"\"\n",
    "        Apply padding to the input array using np.pad().\n",
    "        Default padding is zero-padding.\n",
    "        \"\"\"\n",
    "        return np.pad(x, ((0, 0), (self.padding, self.padding), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Conv1d layer with padding\n",
    "        x: input data with shape (batch_size, input_channels, feature_length)\n",
    "        Returns: output data with shape (batch_size, output_channels, output_length)\n",
    "        \"\"\"\n",
    "        # Apply padding to the input\n",
    "        x_padded = self.pad_input(x)\n",
    "        \n",
    "        batch_size, input_channels, input_length = x_padded.shape\n",
    "        output_length = input_length - self.filter_size + 1  # Ensure output size is calculated correctly\n",
    "        output = np.zeros((batch_size, self.output_channels, output_length), dtype=np.float64)\n",
    "        \n",
    "        # Perform convolution for each output channel, and each sample in the batch\n",
    "        for i in range(self.output_channels):\n",
    "            for b in range(batch_size):  # Process each sample in the batch\n",
    "                for j in range(output_length):  # Iterate over the valid output positions\n",
    "                    # Convolve the filter with the padded input for the b-th sample\n",
    "                    for c in range(input_channels):\n",
    "                        start = j  # Starting index for the slice\n",
    "                        end = start + self.filter_size  # Ending index for the slice\n",
    "                        # Adjust the end index if it goes out of bounds\n",
    "                        if end > input_length + self.padding:\n",
    "                            end = input_length + self.padding\n",
    "                        # Perform the convolution operation\n",
    "                        output[b, i, j] += np.sum(x_padded[b, c, start:end] * self.w[i, c, :end-start])\n",
    "                    # Add bias for the output channel\n",
    "                    output[b, i, j] += self.b[i]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, delta_a, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward pass through the Conv1d layer with padding\n",
    "        x: input data with shape (batch_size, input_channels, feature_length)\n",
    "        delta_a: gradient of the loss with respect to the output, shape (batch_size, output_channels, output_length)\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        # Apply padding to the input for backpropagation\n",
    "        x_padded = self.pad_input(x)\n",
    "        \n",
    "        batch_size, input_channels, input_length = x.shape\n",
    "        delta_w = np.zeros_like(self.w, dtype=np.float64)\n",
    "        delta_b = np.zeros_like(self.b, dtype=np.float64)\n",
    "        \n",
    "        # Initialize delta_x for the backpropagation of each sample in the batch\n",
    "        delta_x = np.zeros_like(x_padded, dtype=np.float64)\n",
    "        \n",
    "        # Gradients for weights, biases, and input (accumulated over the batch)\n",
    "        for i in range(self.output_channels):\n",
    "            for b in range(batch_size):  # Process each sample in the batch\n",
    "                for j in range(delta_a.shape[2]):\n",
    "                    delta_b[i] += delta_a[b, i, j]\n",
    "                    for c in range(input_channels):\n",
    "                        delta_w[i, c] += delta_a[b, i, j] * x_padded[b, c, j:j + self.filter_size].sum()\n",
    "                        delta_x[b, c, j:j + self.filter_size] += delta_a[b, i, j] * self.w[i, c]\n",
    "        \n",
    "        # Remove padding from the gradient w.r.t input (delta_x)\n",
    "        delta_x = delta_x[:, :, self.padding:-self.padding] if self.padding > 0 else delta_x\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.w -= learning_rate * delta_w\n",
    "        self.b -= learning_rate * delta_b\n",
    "        \n",
    "        return delta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output:\n",
      "[[ 9. 16. 22. 17.]\n",
      " [10. 17. 23. 18.]\n",
      " [11. 18. 24. 19.]]\n",
      "Backward pass:\n",
      "Gradient w.r.t input:\n",
      "[[[135. 135.]\n",
      "  [135. 135.]\n",
      "  [  0.   0.]\n",
      "  [  0.   0.]]\n",
      "\n",
      " [[135. 135.]\n",
      "  [135. 135.]\n",
      "  [  0.   0.]\n",
      "  [  0.   0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, input_channels, output_channels, filter_size, padding=0):\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize filters and biases\n",
    "        self.w = np.ones((output_channels, input_channels, filter_size))  # All filters set to 1 for simplicity\n",
    "        self.b = np.array([1, 2, 3], dtype=np.float64)  # Biases initialized as floats\n",
    "        \n",
    "    def pad_input(self, x):\n",
    "        \"\"\"\n",
    "        Apply padding to the input array using np.pad().\n",
    "        Default padding is zero-padding.\n",
    "        \"\"\"\n",
    "        return np.pad(x, ((0, 0), (self.padding, self.padding), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Conv1d layer with padding\n",
    "        x: input data with shape (batch_size, input_channels, feature_length)\n",
    "        Returns: output data with shape (batch_size, output_channels, output_length)\n",
    "        \"\"\"\n",
    "        # Apply padding to the input\n",
    "        x_padded = self.pad_input(x)\n",
    "        \n",
    "        batch_size, input_channels, input_length = x_padded.shape\n",
    "        output_length = input_length - self.filter_size + 1  # Ensure output size is calculated correctly\n",
    "        output = np.zeros((batch_size, self.output_channels, output_length), dtype=np.float64)\n",
    "        \n",
    "        # Perform convolution for each output channel, and each sample in the batch\n",
    "        for i in range(self.output_channels):\n",
    "            for b in range(batch_size):  # Process each sample in the batch\n",
    "                for j in range(output_length):  # Iterate over the valid output positions\n",
    "                    # Convolve the filter with the padded input for the b-th sample\n",
    "                    for c in range(input_channels):\n",
    "                        start = j  # Starting index for the slice\n",
    "                        end = start + self.filter_size  # Ending index for the slice\n",
    "                        # Ensure that the slice doesn't go out of bounds\n",
    "                        if end > input_length + self.padding:\n",
    "                            end = input_length + self.padding\n",
    "                        # Perform the convolution operation\n",
    "                        output[b, i, j] += np.sum(x_padded[b, c, start:end] * self.w[i, c, :end-start])\n",
    "                    # Add bias for the output channel\n",
    "                    output[b, i, j] += self.b[i]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, delta_a, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward pass through the Conv1d layer with padding\n",
    "        x: input data with shape (batch_size, input_channels, feature_length)\n",
    "        delta_a: gradient of the loss with respect to the output, shape (batch_size, output_channels, output_length)\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        # Apply padding to the input for backpropagation\n",
    "        x_padded = self.pad_input(x)\n",
    "        \n",
    "        batch_size, input_channels, input_length = x.shape\n",
    "        delta_w = np.zeros_like(self.w, dtype=np.float64)\n",
    "        delta_b = np.zeros_like(self.b, dtype=np.float64)\n",
    "        \n",
    "        # Initialize delta_x for the backpropagation of each sample in the batch\n",
    "        delta_x = np.zeros_like(x_padded, dtype=np.float64)\n",
    "        \n",
    "        # Gradients for weights, biases, and input (accumulated over the batch)\n",
    "        for i in range(self.output_channels):\n",
    "            for b in range(batch_size):  # Process each sample in the batch\n",
    "                for j in range(delta_a.shape[2]):\n",
    "                    delta_b[i] += delta_a[b, i, j]\n",
    "                    for c in range(input_channels):\n",
    "                        delta_w[i, c] += delta_a[b, i, j] * x_padded[b, c, j:j + self.filter_size].sum()\n",
    "                        delta_x[b, c, j:j + self.filter_size] += delta_a[b, i, j] * self.w[i, c]\n",
    "        \n",
    "        # Remove padding from the gradient w.r.t input (delta_x)\n",
    "        delta_x = delta_x[:, :, self.padding:-self.padding] if self.padding > 0 else delta_x\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.w -= learning_rate * delta_w\n",
    "        self.b -= learning_rate * delta_b\n",
    "        \n",
    "        return delta_x\n",
    "\n",
    "\n",
    "# Example usage with mini-batch:\n",
    "x = np.array([[[1, 2, 3, 4], [2, 3, 4, 5]],  # Batch of 2 samples, 2 channels, 4 features\n",
    "              [[3, 4, 5, 6], [4, 5, 6, 7]]])  # Batch of 2 samples, 2 channels, 4 features\n",
    "w = np.ones((3, 2, 3))  # Filters: 3 output channels, 2 input channels, filter size 3\n",
    "b = np.array([1, 2, 3])  # Biases: 3 output channels\n",
    "\n",
    "# Create Conv1d layer with padding=1 (zero padding)\n",
    "conv_layer = Conv1d(input_channels=2, output_channels=3, filter_size=3, padding=1)\n",
    "\n",
    "# Forward pass\n",
    "print(\"Forward pass output:\")\n",
    "print(output)\n",
    "\n",
    "# Backpropagation (dummy delta_a for demonstration)\n",
    "delta_a = np.array([[[10, 20], [15, 25], [30, 35]],  # Gradient w.r.t output for each sample\n",
    "                    [[10, 20], [15, 25], [30, 35]]])  # Gradient w.r.t output for each sample\n",
    "delta_x = conv_layer.backward(x, delta_a, learning_rate=0.01)\n",
    "\n",
    "print(\"Backward pass:\")\n",
    "print(\"Gradient w.r.t input:\")\n",
    "print(delta_x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
